{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0e932ab",
   "metadata": {},
   "source": [
    "## IMPORT MODULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968c46a4",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from selection_service.enums.Enums import DesignCode, ProviderName\n",
    "from selection_service.core.Pipeline import EarthquakeAPI\n",
    "from selection_service.providers.ProvidersFactory import ProviderFactory\n",
    "from selection_service.processing.Selection import SelectionConfig,SearchCriteria,TBDYSelectionStrategy,TargetParameters\n",
    "from selection_service.core.LoggingConfig import setup_logging\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a556f56e",
   "metadata": {},
   "source": [
    "## PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b33b3c",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "setup_logging(log_level= logging.INFO, log_dir= \"logs\")\n",
    "con = SelectionConfig(design_code=DesignCode.TBDY_2018, num_records=22, max_per_station=3, max_per_event=3, min_score=55)\n",
    "strategy = TBDYSelectionStrategy(config=con)\n",
    "search_criteria = SearchCriteria(start_date=\"2020-01-01\", end_date=\"2025-09-05\", min_magnitude=7.0, max_magnitude=10.0, min_vs30=400, max_vs30=500 )\n",
    "target_params = TargetParameters(magnitude=7.0, distance=30.0, vs30=400.0, pga=300, mechanism=[\"StrikeSlip\",\"Reverse\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf98ea",
   "metadata": {},
   "source": [
    "## API FUNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118b59ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = EarthquakeAPI(providerNames=[ProviderName.AFAD,\n",
    "                                   ProviderName.PEER],\n",
    "                    strategies=[strategy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a37cbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = api.run_sync(criteria=search_criteria,\n",
    "                      target=target_params,\n",
    "                      strategy_name=strategy.get_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09d6120",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultAsync = await api.run_async(criteria=search_criteria,\n",
    "                            target=target_params,\n",
    "                            strategy_name=strategy.get_name())\n",
    "resultAsync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfea269",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultAsync.value.scored_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2ce828",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0b4fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(result.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c24aab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aedbb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.value.failed_providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6774b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.value.execution_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bdb700",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.value.logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39219051",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0011a36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.value.selected_df[['PROVIDER','RSN','EVENT','YEAR','MAGNITUDE','MAGNITUDE_TYPE','STATION','VS30(m/s)','RRUP(km)','MECHANISM','T90_avg(sec)','PGA(cm2/sec)','PGV(cm/sec)','SCORE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64219587",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.value.scored_df[['PROVIDER','RSN','EVENT','YEAR','MAGNITUDE','STATION','VS30(m/s)','RRUP(km)','MECHANISM','T90_avg(sec)','PGA(cm2/sec)','PGV(cm/sec)','SCORE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9010f99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.value.scored_df.sort_values(by=['SCORE'],ascending=True)[['PROVIDER','RSN','EVENT','YEAR','MAGNITUDE','STATION','VS30(m/s)','RRUP(km)','MECHANISM','T90_avg(sec)','PGA(cm2/sec)','PGV(cm/sec)','SCORE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f8458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.value.scored_df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f1b015",
   "metadata": {},
   "source": [
    "## FDSN Provider Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412935cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "BASE_URL = \"https://service.iris.edu/fdsnws/event/1/query\"\n",
    "\n",
    "# Tarih aralığı: son 30 gün\n",
    "endtime = datetime.utcnow()\n",
    "starttime = endtime - timedelta(days=30)\n",
    "\n",
    "params = {\n",
    "    \"starttime\": starttime.strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "    \"endtime\": endtime.strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "    \"minmagnitude\": 4.0,\n",
    "    \"minlatitude\": 35,\n",
    "    \"maxlatitude\": 42,\n",
    "    \"minlongitude\": 25,\n",
    "    \"maxlongitude\": 45,\n",
    "    \"orderby\": \"time\"\n",
    "    # \"format\": \"quakeml\"  # <-- bunu kaldırdık\n",
    "}\n",
    "\n",
    "response = requests.get(BASE_URL, params=params)\n",
    "response.raise_for_status()\n",
    "\n",
    "# XML parse\n",
    "root = ET.fromstring(response.text)\n",
    "\n",
    "# QuakeML namespace\n",
    "ns = {\"q\": \"http://quakeml.org/xmlns/bed/1.2\"}\n",
    "\n",
    "events = []\n",
    "for event in root.findall(\".//q:event\", ns):\n",
    "    origin = event.find(\".//q:origin\", ns)\n",
    "    magnitude = event.find(\".//q:magnitude\", ns)\n",
    "    \n",
    "    if origin is not None and magnitude is not None:\n",
    "        time_el = origin.find(\".//q:time/q:value\", ns)\n",
    "        lat_el = origin.find(\".//q:latitude/q:value\", ns)\n",
    "        lon_el = origin.find(\".//q:longitude/q:value\", ns)\n",
    "        depth_el = origin.find(\".//q:depth/q:value\", ns)\n",
    "        mag_el = magnitude.find(\".//q:mag/q:value\", ns)\n",
    "\n",
    "        events.append({\n",
    "            \"time\": time_el.text if time_el is not None else None,\n",
    "            \"latitude\": float(lat_el.text) if lat_el is not None else None,\n",
    "            \"longitude\": float(lon_el.text) if lon_el is not None else None,\n",
    "            \"depth_km\": float(depth_el.text)/1000 if depth_el is not None else None,\n",
    "            \"magnitude\": float(mag_el.text) if mag_el is not None else None\n",
    "        })\n",
    "\n",
    "for ev in events:\n",
    "    print(f\"{ev['time']} | Mw {ev['magnitude']} | \"\n",
    "          f\"Lat {ev['latitude']}, Lon {ev['longitude']}, Depth {ev['depth_km']} km\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b00bc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "BASE_URL = \"https://service.iris.edu/fdsnws/event/1/query\"\n",
    "\n",
    "# Tarih aralığı: son 30 gün\n",
    "endtime = datetime.utcnow()\n",
    "starttime = endtime - timedelta(days=30)\n",
    "\n",
    "params = {\n",
    "    \"starttime\": starttime.strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "    \"endtime\": endtime.strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
    "    \"minmagnitude\": 4.0,\n",
    "    \"minlatitude\": 35,\n",
    "    \"maxlatitude\": 42,\n",
    "    \"minlongitude\": 25,\n",
    "    \"maxlongitude\": 45,\n",
    "    \"orderby\": \"time\",\n",
    "    \"format\": \"geojson\"\n",
    "}\n",
    "\n",
    "response = requests.get(BASE_URL, params=params)\n",
    "response.raise_for_status()\n",
    "data = response.json()\n",
    "\n",
    "# Deprem bilgilerini çekelim\n",
    "events = []\n",
    "for feature in data.get(\"features\", []):\n",
    "    props = feature.get(\"properties\", {})\n",
    "    coords = feature.get(\"geometry\", {}).get(\"coordinates\", [None, None, None])\n",
    "    \n",
    "    events.append({\n",
    "        \"time\": datetime.utcfromtimestamp(props.get(\"time\", 0) / 1000),\n",
    "        \"magnitude\": props.get(\"mag\"),\n",
    "        \"place\": props.get(\"place\"),\n",
    "        \"longitude\": coords[0],\n",
    "        \"latitude\": coords[1],\n",
    "        \"depth_km\": coords[2]\n",
    "    })\n",
    "\n",
    "# DataFrame'e dökelim\n",
    "df = pd.DataFrame(events)\n",
    "print(df)\n",
    "\n",
    "# CSV'ye kaydetmek istersen:\n",
    "# df.to_csv(\"earthquakes_turkey_last30days.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bead8e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "BASE_URL = \"https://earthquake.usgs.gov/fdsnws/event/1/query\"\n",
    "\n",
    "# Son 30 gün\n",
    "endtime = datetime.utcnow()\n",
    "starttime = endtime - timedelta(days=30)\n",
    "\n",
    "params = {\n",
    "    \"format\": \"geojson\",\n",
    "    \"starttime\": starttime.strftime(\"%Y-%m-%d\"),\n",
    "    \"endtime\": endtime.strftime(\"%Y-%m-%d\"),\n",
    "    \"minmagnitude\": 4.0,\n",
    "    \"minlatitude\": 35,\n",
    "    \"maxlatitude\": 42,\n",
    "    \"minlongitude\": 25,\n",
    "    \"maxlongitude\": 45,\n",
    "    \"orderby\": \"time\"\n",
    "}\n",
    "\n",
    "response = requests.get(BASE_URL, params=params)\n",
    "response.raise_for_status()\n",
    "data = response.json()\n",
    "\n",
    "events = []\n",
    "for feature in data.get(\"features\", []):\n",
    "    props = feature.get(\"properties\", {})\n",
    "    coords = feature.get(\"geometry\", {}).get(\"coordinates\", [None, None, None])\n",
    "    \n",
    "    events.append({\n",
    "        \"time\": datetime.utcfromtimestamp(props.get(\"time\", 0) / 1000),\n",
    "        \"magnitude\": props.get(\"mag\"),\n",
    "        \"place\": props.get(\"place\"),\n",
    "        \"longitude\": coords[0],\n",
    "        \"latitude\": coords[1],\n",
    "        \"depth_km\": coords[2]\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(events)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b51d4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class EarthquakeClient:\n",
    "    PROVIDERS = {\n",
    "        \"IRIS\": {\n",
    "            \"url\": \"https://service.iris.edu/fdsnws/event/1/query\",\n",
    "            \"format\": \"xml\"\n",
    "        },\n",
    "        \"USGS\": {\n",
    "            \"url\": \"https://earthquake.usgs.gov/fdsnws/event/1/query\",\n",
    "            \"format\": \"geojson\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def __init__(self, provider: str = \"IRIS\"):\n",
    "        if provider not in self.PROVIDERS:\n",
    "            raise ValueError(f\"Desteklenmeyen provider: {provider}\")\n",
    "        self.provider = provider\n",
    "        self.base_url = self.PROVIDERS[provider][\"url\"]\n",
    "        self.format = self.PROVIDERS[provider][\"format\"]\n",
    "\n",
    "    def search(self, **kwargs) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Deprem araması yapar, DataFrame döner.\n",
    "        Parametreler provider’a göre otomatik işlenir.\n",
    "        \"\"\"\n",
    "        if self.provider == \"USGS\":\n",
    "            return self._search_usgs(**kwargs)\n",
    "        elif self.provider == \"IRIS\":\n",
    "            return self._search_iris(**kwargs)\n",
    "\n",
    "    def _search_usgs(self, **kwargs) -> pd.DataFrame:\n",
    "        params = {\"format\": \"geojson\"}\n",
    "        params.update(kwargs)\n",
    "\n",
    "        r = requests.get(self.base_url, params=params)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "\n",
    "        events = []\n",
    "        for feature in data.get(\"features\", []):\n",
    "            props = feature.get(\"properties\", {})\n",
    "            coords = feature.get(\"geometry\", {}).get(\"coordinates\", [None, None, None])\n",
    "            events.append({\n",
    "                \"time\": datetime.utcfromtimestamp(props.get(\"time\", 0) / 1000),\n",
    "                \"magnitude\": props.get(\"mag\"),\n",
    "                \"place\": props.get(\"place\"),\n",
    "                \"longitude\": coords[0],\n",
    "                \"latitude\": coords[1],\n",
    "                \"depth_km\": coords[2]\n",
    "            })\n",
    "\n",
    "        return pd.DataFrame(events)\n",
    "\n",
    "    def _search_iris(self, **kwargs) -> pd.DataFrame:\n",
    "        params = kwargs.copy()  # IRIS zaten XML (QuakeML) döner\n",
    "        r = requests.get(self.base_url, params=params)\n",
    "        r.raise_for_status()\n",
    "        root = ET.fromstring(r.text)\n",
    "\n",
    "        ns = {\"q\": \"http://quakeml.org/xmlns/bed/1.2\"}\n",
    "        events = []\n",
    "        for event in root.findall(\".//q:event\", ns):\n",
    "            origin = event.find(\".//q:origin\", ns)\n",
    "            magnitude = event.find(\".//q:magnitude\", ns)\n",
    "            if origin is None or magnitude is None:\n",
    "                continue\n",
    "            time_el = origin.find(\".//q:time/q:value\", ns)\n",
    "            lat_el = origin.find(\".//q:latitude/q:value\", ns)\n",
    "            lon_el = origin.find(\".//q:longitude/q:value\", ns)\n",
    "            depth_el = origin.find(\".//q:depth/q:value\", ns)\n",
    "            mag_el = magnitude.find(\".//q:mag/q:value\", ns)\n",
    "\n",
    "            events.append({\n",
    "                \"time\": time_el.text if time_el is not None else None,\n",
    "                \"latitude\": float(lat_el.text) if lat_el is not None else None,\n",
    "                \"longitude\": float(lon_el.text) if lon_el is not None else None,\n",
    "                \"depth_km\": float(depth_el.text) / 1000 if depth_el is not None else None,\n",
    "                \"magnitude\": float(mag_el.text) if mag_el is not None else None\n",
    "            })\n",
    "\n",
    "        return pd.DataFrame(events)\n",
    "\n",
    "# USGS üzerinden GeoJSON (Türkiye sınırları, son 7 gün, Mw >= 4.5)\n",
    "client = EarthquakeClient(provider=\"USGS\")\n",
    "df = client.search(\n",
    "    starttime=\"2025-09-18\",\n",
    "    endtime=\"2025-09-25\",\n",
    "    minmagnitude=4.5,\n",
    "    minlatitude=35,\n",
    "    maxlatitude=42,\n",
    "    minlongitude=25,\n",
    "    maxlongitude=45,\n",
    "    orderby=\"time\"\n",
    ")\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "# IRIS üzerinden QuakeML (aynı parametreler)\n",
    "client = EarthquakeClient(provider=\"IRIS\")\n",
    "df2 = client.search(\n",
    "    starttime=\"2025-09-18T00:00:00\",\n",
    "    endtime=\"2025-09-25T23:59:59\",\n",
    "    minmagnitude=4.5,\n",
    "    minlatitude=35,\n",
    "    maxlatitude=42,\n",
    "    minlongitude=25,\n",
    "    maxlongitude=45,\n",
    "    orderby=\"time\"\n",
    ")\n",
    "print(df2.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48dda25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy.clients.fdsn import Client\n",
    "fdsn = Client(\"IRIS\")\n",
    "catalog = fdsn.get_events(starttime=\"2020-01-01\", endtime=\"2025-09-05\", minmagnitude=7.0, maxmagnitude=10.0)\n",
    "catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d14a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog.events[0].preferred_origin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e768c66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog.events[0].preferred_magnitude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f86042",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog.events[0].preferred_focal_mechanism()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9d45a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy import UTCDateTime\n",
    "\n",
    "\n",
    "t1 = UTCDateTime(\"2010-02-27T06:30:00.000\")\n",
    "t2 = t1 + 5\n",
    "st = fdsn.get_waveforms(\"IU\", \"ANMO\", \"00\", \"LHZ\", t1, t2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c660fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.traces[0].stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05149c4",
   "metadata": {},
   "source": [
    "## DOWNLOAD AFAD WAVEFORMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d0d1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.value.selected_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68adfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "afad_filenames = result.value.selected_df[result.value.selected_df['PROVIDER'] == \"AFAD\"]['FILE_NAME_H1'].to_list()\n",
    "afad_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e16cf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "afad_provider = api.providers[0]\n",
    "response= afad_provider.download_afad_waveforms_batch(afad_filenames,file_status=\"RawAcc\", batch_size=5, event_id=\"TEST_EVENT\", user_name=\"GuestUser\", export_type= \"mseed\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf2dc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0257ef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path=\"D:\\\\github\\\\SelectionEarthquake\\\\examples\\\\Afad_events\\\\TEST_EVENT\\\\0213\\\\20230206011732_0213.mseed\"\n",
    "file_path=\"D:\\\\github\\\\SelectionEarthquake\\\\examples\\\\Afad_events\\\\TEST_EVENT\\\\0213\\\\20230206011732_0213.mseed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfc7ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mseed_reader.py\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import Dict, List\n",
    "\n",
    "class GroundMotionReader():\n",
    "    \"\"\"\n",
    "    AFAD'ın metin tabanlı deprem veri formatını okuyan sınıf.\n",
    "    \"\"\"\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.metadata = {}\n",
    "        self.data = None\n",
    "        self.units = \"gal\"\n",
    "        self.fs = 100.0  # Varsayılan örnekleme frekansı (0.01 s → 100 Hz)\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"Veri dosyasını okur ve verileri hazırlar.\"\"\"\n",
    "        self._parse_file()\n",
    "\n",
    "    def _parse_file(self):\n",
    "        \"\"\"Dosyayı satır satır okuyarak metadata ve verileri ayırır.\"\"\"\n",
    "        with open(self.file_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        metadata_lines = []\n",
    "        data_lines = []\n",
    "        in_data_section = False\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            if line.startswith(('N-S', 'E-W', 'U-D')) and not in_data_section:\n",
    "                # Veri bölümü başlığı\n",
    "                in_data_section = True\n",
    "                continue\n",
    "                \n",
    "            if in_data_section:\n",
    "                # Veri satırları\n",
    "                if re.match(r'^-?\\d+\\.\\d+', line):\n",
    "                    data_lines.append(line)\n",
    "            else:\n",
    "                # Metadata satırları\n",
    "                metadata_lines.append(line)\n",
    "        \n",
    "        # Metadata'ları parse et\n",
    "        self._parse_metadata(metadata_lines)\n",
    "        \n",
    "        # Verileri parse et\n",
    "        self._parse_data(data_lines)\n",
    "\n",
    "    def _parse_metadata(self, lines: List[str]):\n",
    "        \"\"\"Metadata satırlarını parse eder.\"\"\"\n",
    "        for line in lines:\n",
    "            if ':' in line:\n",
    "                key, value = line.split(':', 1)\n",
    "                key = key.strip()\n",
    "                value = value.strip()\n",
    "                self.metadata[key] = value\n",
    "                \n",
    "                # Önemli bilgileri ayıkla\n",
    "                if 'SAMPLING INTERVAL' in key:\n",
    "                    try:\n",
    "                        interval = float(value.split()[0])\n",
    "                        self.fs = 1.0 / interval  # Örnekleme frekansı\n",
    "                    except:\n",
    "                        pass\n",
    "                elif 'NUMBER OF DATA' in key:\n",
    "                    try:\n",
    "                        self.n_data = int(value)\n",
    "                    except:\n",
    "                        pass\n",
    "                elif 'RAW PGA VALUES' in key:\n",
    "                    # PGA değerlerini ayıkla\n",
    "                    pga_values = re.findall(r'\\(([NSEWUD\\-]+)\\)\\s+([\\d\\.]+)', value)\n",
    "                    for direction, value in pga_values:\n",
    "                        self.metadata[f'PGA_{direction}'] = float(value)\n",
    "\n",
    "    def _parse_data(self, lines: List[str]):\n",
    "        \"\"\"Veri satırlarını parse eder.\"\"\"\n",
    "        ns_data = []\n",
    "        ew_data = []\n",
    "        ud_data = []\n",
    "        \n",
    "        for line in lines:\n",
    "            # Satırdaki 3 float değerini ayıkla\n",
    "            values = re.findall(r'-?\\d+\\.\\d+', line)\n",
    "            if len(values) == 3:\n",
    "                ns_data.append(float(values[0]))\n",
    "                ew_data.append(float(values[1]))\n",
    "                ud_data.append(float(values[2]))\n",
    "        \n",
    "        # NumPy array'lerine dönüştür\n",
    "        self.data = {\n",
    "            'NS': np.array(ns_data),\n",
    "            'EW': np.array(ew_data),\n",
    "            'UD': np.array(ud_data)\n",
    "        }\n",
    "        \n",
    "        # Zaman vektörünü oluştur\n",
    "        n_samples = len(ns_data)\n",
    "        self.time = np.arange(0, n_samples / self.fs, 1 / self.fs)\n",
    "\n",
    "    def get_data(self) -> Dict:\n",
    "        \"\"\"Parselenen verileri döndürür.\"\"\"\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"Önce start() metodunu çağırın\")\n",
    "        \n",
    "        return {\n",
    "            \"direction\": ['NS', 'EW', 'UD'],\n",
    "            \"time\": self.time,\n",
    "            \"acc\": np.array([self.data['NS'], self.data['EW'], self.data['UD']]),\n",
    "            \"units\": self.units,\n",
    "            \"fs\": self.fs,\n",
    "            \"meta_data\": self.metadata,\n",
    "            \"n_samples\": len(self.time)\n",
    "        }\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"Temizlik işlemleri.\"\"\"\n",
    "        pass\n",
    "\n",
    "# Test\n",
    "\n",
    "try:\n",
    "    reader = GroundMotionReader(file_path)\n",
    "    reader.start()\n",
    "    data = reader.get_data()\n",
    "    \n",
    "    print(\"✓ Veri başarıyla okundu!\")\n",
    "    print(f\"Örnek sayısı: {data['n_samples']}\")\n",
    "    print(f\"Örnekleme frekansı: {data['fs']} Hz\")\n",
    "    print(f\"Birimler: {data['units']}\")\n",
    "    print(f\"Zaman aralığı: {data['time'][0]:.2f} - {data['time'][-1]:.2f} s\")\n",
    "    print(f\"NS veri boyutu: {data['acc'][0].shape}\")\n",
    "    print(f\"EW veri boyutu: {data['acc'][1].shape}\")\n",
    "    print(f\"UD veri boyutu: {data['acc'][2].shape}\")\n",
    "    \n",
    "    # İlk 5 örnek\n",
    "    print(\"\\nİlk 5 örnek:\")\n",
    "    for i in range(5):\n",
    "        print(f\"{data['time'][i]:6.3f}s: NS={data['acc'][0][i]:8.3f}, EW={data['acc'][1][i]:8.3f}, UD={data['acc'][2][i]:8.3f} {data['units']}\")\n",
    "    \n",
    "    # Metadata\n",
    "    print(\"\\nÖnemli Metadata:\")\n",
    "    for key in ['PLACE', 'EARTHQUAKE DATE', 'EARTHQUAKE MAGNITUDE', 'STATION ID']:\n",
    "        if key in data['meta_data']:\n",
    "            print(f\"{key}: {data['meta_data'][key]}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"✗ Hata: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e45b0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(data['acc'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ce8736",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from selection_service.processing.Selection import SearchCriteria,TargetParameters\n",
    "from selection_service.enums.Enums import ProviderName\n",
    "\n",
    "prvFactory = ProviderFactory()\n",
    "fdsnProvider = prvFactory.create_provider(ProviderName.FDSN)\n",
    "search_crit = SearchCriteria(start_date=\"2020-01-01\", end_date=\"2025-09-05\", min_magnitude=7.0, max_magnitude=10.0, min_vs30=400, max_vs30=500 )\n",
    "result = await fdsnProvider.fetch_data_async(criteria=search_crit)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1a6978",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.value.iloc()[0]['EQID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203d4099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy import UTCDateTime\n",
    "from obspy.clients.fdsn import Client\n",
    "\n",
    "# IRIS istemcisini oluştur\n",
    "client = Client(\"IRIS\")\n",
    "\n",
    "# Deprem parametrelerini belirle\n",
    "starttime = UTCDateTime(\"2024-01-01T00:00:00\")\n",
    "endtime = UTCDateTime(\"2025-01-02T00:00:00\")\n",
    "min_latitude = 35.0\n",
    "max_latitude = 42.0\n",
    "min_longitude = 25.0\n",
    "max_longitude = 45.0\n",
    "min_magnitude = 4.0\n",
    "\n",
    "# Deprem kataloğunu çek\n",
    "catalog = client.get_events(\n",
    "    starttime=starttime,\n",
    "    endtime=endtime,\n",
    "    minlatitude=min_latitude,\n",
    "    maxlatitude=max_latitude,\n",
    "    minlongitude=min_longitude,\n",
    "    maxlongitude=max_longitude,\n",
    "    minmagnitude=min_magnitude\n",
    ")\n",
    "\n",
    "print(f\"Bulunan deprem sayısı: {len(catalog)}\")\n",
    "\n",
    "# Depremleri listele\n",
    "for i, event in enumerate(catalog):\n",
    "    origin = event.preferred_origin()\n",
    "    magnitude = event.preferred_magnitude()\n",
    "    print(f\"{i+1}. Deprem:\")\n",
    "    print(f\"   Zaman: {origin.time}\")\n",
    "    print(f\"   Enlem: {origin.latitude:.2f}\")\n",
    "    print(f\"   Boylam: {origin.longitude:.2f}\")\n",
    "    print(f\"   Derinlik: {origin.depth/1000:.1f} km\")\n",
    "    print(f\"   Büyüklük: {magnitude.mag}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16247a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(catalog[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7199ba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(catalog[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3838138",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3829da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy import UTCDateTime\n",
    "from obspy.clients.fdsn import Client\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def deprem_dalga_formu_cek(deprem_zamani, istasyon, sure=300):\n",
    "    \"\"\"Belirli bir depremin dalga formunu çek\"\"\"\n",
    "    client = Client(\"IRIS\")\n",
    "    \n",
    "    # Depremden sonraki dalga formlarını çek\n",
    "    start = deprem_zamani - 30  # Depremden 30 saniye önce\n",
    "    end = start + sure  # Toplam süre\n",
    "    \n",
    "    try:\n",
    "        # Dalga formu verisini çek\n",
    "        stream = client.get_waveforms(\n",
    "            network=\"IU\",  # Global istasyon ağı\n",
    "            station=istasyon,\n",
    "            location=\"00\",\n",
    "            channel=\"BHZ\",  # Düşey bileşen\n",
    "            starttime=start,\n",
    "            endtime=end\n",
    "        )\n",
    "        \n",
    "        # Veriyi işle\n",
    "        stream.detrend('linear')\n",
    "        stream.taper(max_percentage=0.05)\n",
    "        stream.filter('bandpass', freqmin=0.01, freqmax=1.0)\n",
    "        \n",
    "        # Grafik çiz\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for i, trace in enumerate(stream):\n",
    "            plt.plot(trace.times(), trace.data, label=trace.id)\n",
    "        \n",
    "        plt.axvline(x=30, color='red', linestyle='--', label='Deprem Zamanı')\n",
    "        plt.xlabel('Zaman (s)')\n",
    "        plt.ylabel('Genlik')\n",
    "        plt.title(f'Deprem Dalga Formu - İstasyon: {istasyon}')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        return stream\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Hata: {e}\")\n",
    "        return None\n",
    "\n",
    "# Örnek kullanım\n",
    "deprem_zamani = UTCDateTime(\"2024-01-01T10:30:00\")\n",
    "deprem_dalga_formu_cek(deprem_zamani,None, 300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
